{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Sharing Demand Forecast\n",
    "\n",
    "Data source: https://www.kaggle.com/c/bike-sharing-demand/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check & clean imports\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression, ElasticNet\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, r2_score, classification_report, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Business Goal\n",
    "\n",
    "Forecast bike demand given the data: datetime, season, holiday, workingday, weather, temp, atemp, humidity, windspeed.\n",
    "\n",
    "Example in words: Given the forecasted weather conditions, how many bicycles can we expect to be rented out (city-wide) this Saturday at 2pm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/bike-sharing-demand/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime format string doc: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(keys='datetime', drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**count** is the sum of:\n",
    "* **casual** - number of non-registered user rentals initiated\n",
    "* **registered** - number of registered user rentals initiated\n",
    "\n",
    "It belongs to our y data (target data), so we can omit it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['casual', 'registered'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train-Test-Split\n",
    "\n",
    "Define X and y:\n",
    "\n",
    "* X : Training data\n",
    "* y : Target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.DataFrame(df) # keep a deep copy of unsplit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['count']\n",
    "X = df.drop(['count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore the Data\n",
    "\n",
    "* **???** Explore on the *df* or the split data (problem of keeping track of y)\n",
    "* **???** Explore time series *after* splitting? It will introduce random holes in equal sampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge for exploration\n",
    "df_train = X_train.join(other=y_train)\n",
    "#df = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count'].plot(figsize=(12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The demand contains a positive trend over the years. Possible strategies may be applied:\n",
    "\n",
    "* https://machinelearningmastery.com/time-series-trends-in-python/\n",
    "* https://www.investopedia.com/terms/d/detrend.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:1000]['count'].plot(figsize=(12,5), style='.-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mind the data gaps\n",
    "\n",
    "*\\\"For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month.\\\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include more data related to time and weather and moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = df.iloc[7000:8500]\n",
    "window = '24H'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "sns.lineplot(ax=ax, data=subdf.rolling(window).mean(), x=df.index.name, y='count', color='k', legend='brief', alpha=0.7)\n",
    "sns.scatterplot(ax=ax, data=subdf, x=df.index.name, y='count', hue='workingday', alpha=0.65)\n",
    "ax.legend(['Avg ({})'.format(window), 'Working Day', 'No Working Day'])\n",
    "ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing work to non-work days you can see all scenarios:\n",
    "* more than local avg demand\n",
    "* same ...\n",
    "* less ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = df.iloc[7000:8500]\n",
    "window = '24H'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "sns.lineplot(ax=ax, data=subdf.rolling(window).mean(), x=df.index.name, y='count', color='k', legend='brief', alpha=0.7)\n",
    "sns.scatterplot(ax=ax, data=subdf, x=df.index.name, y='count', hue='weather', alpha=0.65)\n",
    "ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demand dips on days with bad weather.\n",
    "\n",
    "Accessing DataFrames by datetime: https://www.dataquest.io/blog/tutorial-time-series-analysis-with-pandas/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Type 4 almost not present in data\n",
    "\n",
    "* 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "* 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "* 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "* 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weather'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['weather'] > 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = df.rolling('24H').mean()\n",
    "ax = sns.scatterplot(       data=df   , y='count', x='temp', alpha=0.1)\n",
    "ax = sns.scatterplot(ax=ax, data=subdf, y='count', x='temp', alpha=0.1)\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = 'atemp'\n",
    "subdf = df.rolling('24H').mean()\n",
    "ax = sns.scatterplot(       data=df   , y='count', x=feat, alpha=0.1)\n",
    "ax = sns.scatterplot(ax=ax, data=subdf, y='count', x=feat, alpha=0.1)\n",
    "ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher temps -> higher demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = 'humidity'\n",
    "subdf = df.rolling('24H').mean()\n",
    "ax = sns.scatterplot(       data=df   , y='count', x=feat, alpha=0.1)\n",
    "ax = sns.scatterplot(ax=ax, data=subdf, y='count', x=feat, alpha=0.1)\n",
    "ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Humidity above 80 goes with a decrease in demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = 'windspeed'\n",
    "subdf = df.rolling('24H').mean()\n",
    "ax = sns.scatterplot(       data=df   , y='count', x=feat, alpha=0.1)\n",
    "ax = sns.scatterplot(ax=ax, data=subdf, y='count', x=feat, alpha=0.1)\n",
    "ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High windspeeds go with decrease in demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas\n",
    "\n",
    "* Bin humidity/temps and calculate mean deamand for each range. (With help of sklearn preprocessors)\n",
    "* temp vs atemp correlation: does it actually contain additional information?\n",
    "* use season, hoiday, workday, wheathertype\n",
    "* use resample method (props to catarina)\n",
    "* use conditional probability/thinking: e.g. if wheather = 1 ==> stronger correlation between temp and demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning by temperature and plot mean demand\n",
    "df['temp_b'] = pd.cut(x=df['temp'], bins=int((df['temp'].max()-df['temp'].min())/1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf=df[['temp_b','count']].groupby('temp_b').mean()\n",
    "subdf['count'].plot(figsize=(18,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18,5))\n",
    "# thanks to alisa\n",
    "sns.boxplot(ax=ax, data=df[['temp_b','count']], x='temp_b', y='count')\n",
    "ax.set_xticklabels(labels=ax.get_xticklabels(), rotation=90)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning by windspeed and plot mean demand\n",
    "df['windspeed_b'] = pd.cut(x=df['windspeed'], bins=10)\n",
    "subdf = df[['windspeed_b','count']].groupby('windspeed_b').mean()\n",
    "subdf['count'].plot(figsize=(28,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelling\n",
    "### 5.1 Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add time related features\n",
    "\n",
    "* Day number since unix epoch (for implicit detrend of demand)\n",
    "* Week day, hour, day of month  (one-hot-encoded and/or ordinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional features\n",
    "\n",
    "* Interaction terms, Polynomial terms\n",
    "\n",
    "#### Helpful material\n",
    "* datetime docs: https://docs.python.org/3/library/datetime.html#datetime.datetime.timestamp\n",
    "* datetime in pandas: https://towardsdatascience.com/working-with-datetime-in-pandas-dataframe-663f7af6c587"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considerations\n",
    "\n",
    "* Hour, dayofweek, month -> numeric or one-hot-encoded ?\n",
    "* Wheather -> num and/or one-hot ?\n",
    "\n",
    "#### Further ideas\n",
    "\n",
    "* StandardScaler after polynomial/interaction expansion\n",
    "* Bin and one-hot-encode continuous variables like temperature.\n",
    "* Introduce new one-hot-encoded time variable: Quarter of the day.\n",
    "* Remove features and look at the impact.\n",
    "* Hyperparameter optimization and evaluation\n",
    "    * Other regression types (Ridge, ElasticNet)\n",
    "    * Use *GridSearchCV*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_to_hour(df):\n",
    "    df.iloc[:,0] = df.iloc[:,0].dt.hour\n",
    "    return df\n",
    "\n",
    "def expand_to_dayofweek(df):\n",
    "    df.iloc[:,0] = df.iloc[:,0].dt.dayofweek\n",
    "    return df\n",
    "\n",
    "def expand_to_month(df):\n",
    "    df.iloc[:,0] = df.iloc[:,0].dt.month\n",
    "    return df\n",
    "\n",
    "def expand_to_quarter(df):\n",
    "    df.iloc[:,0] = df.iloc[:,0].dt.quarter\n",
    "    return df\n",
    "\n",
    "def expand_to_year(df):\n",
    "    df.iloc[:,0] = df.iloc[:,0].dt.year\n",
    "    return df\n",
    "\n",
    "def expand_to_timestamp(df, since=\"2000/01/01\"):\n",
    "    \"Timestamp in days\"\n",
    "    timedelta = df.iloc[:,0] - pd.to_datetime(since)\n",
    "    DAY_IN_SECONDS = 60*60*24\n",
    "    df.iloc[:,0] = timedelta.dt.total_seconds()/(DAY_IN_SECONDS)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features_numerical_encoded = []\n",
    "for expand_func in [expand_to_hour, expand_to_dayofweek, expand_to_month\n",
    "                    ,expand_to_year, expand_to_quarter, expand_to_timestamp]:\n",
    "    item = make_pipeline(FunctionTransformer(func=expand_func)\n",
    "                         ,StandardScaler()) , ['datetime']\n",
    "    time_features_numerical_encoded.append(list(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features_one_hot_encoded = [\n",
    "    # Quarter of the day in one-hot-encoding\n",
    "    [ make_pipeline(FunctionTransformer(func=expand_to_hour)\n",
    "                    ,KBinsDiscretizer(n_bins=4, strategy='uniform'\n",
    "                                       ,encode='onehot')), ['datetime'] ]\n",
    "]\n",
    "for expand_func in [expand_to_hour, expand_to_dayofweek, expand_to_month\n",
    "                    ,expand_to_year, expand_to_quarter]:\n",
    "    item = make_pipeline(FunctionTransformer(func=expand_func)\n",
    "                         ,OneHotEncoder(sparse=False)) , ['datetime']\n",
    "    time_features_one_hot_encoded.append(list(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_continuous_features = [[ StandardScaler() , ['windspeed', 'humidity', 'temp', 'atemp'] ]]\n",
    "day_category_features = [[ OneHotEncoder(sparse=False) , ['workingday', 'holiday'] ]]\n",
    "\n",
    "wheather_feature_numerical = [[ StandardScaler() , ['weather'] ]]\n",
    "wheather_feature_ohe = [[ OneHotEncoder(sparse=False) , ['weather'] ]]\n",
    "\n",
    "season_feature_numerical = [[ StandardScaler() , ['season'] ]]\n",
    "season_feature_ohe = [[ OneHotEncoder(sparse=False) , ['season'] ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_sets = [\n",
    "    time_features_numerical_encoded\n",
    "    ,time_features_one_hot_encoded\n",
    "    ,physical_continuous_features\n",
    "    ,day_category_features\n",
    "    ,wheather_feature_numerical\n",
    "    ,wheather_feature_ohe\n",
    "    ,season_feature_numerical\n",
    "    ,season_feature_ohe\n",
    "]\n",
    "all_features = []\n",
    "[all_features.extend(features) for features in all_feature_sets]\n",
    "column_transformer = make_column_transformer(*all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: Try different combinations of `all_feature_sets` with the help of `itertools.combinations()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = make_pipeline( column_transformer\n",
    "                            ,PolynomialFeatures(degree=2) )\n",
    "#transformer = column_transformer # override PolynomialFeatures\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = transformer.fit_transform(X_train)\n",
    "#for i in range(30):\n",
    "#    print(dd[i][1],dd[i][-4:])\n",
    "dd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.fit(X_train)\n",
    "X_train_tf = transformer.transform(X_train)\n",
    "X_test_tf  = transformer.transform(X_test)\n",
    "X_train_tf.shape, X_test_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Train model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net_m = ElasticNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net_m.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'alpha': np.linspace(0,1,21),\n",
    "    'l1_ratio': np.linspace(0,1,11)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = GridSearchCV(estimator=elastic_net_m, param_grid=hyperparams, cv=5, scoring='r2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tol = 0.01\n",
    "(m.coef_ < _tol).sum()#, m.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Show features that got selected by L1 regularization (# are non-zero coeffs):\")\n",
    "for coef in m.coef_:\n",
    "    print(\"#\" if coef > _tol else \"_\", end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = m.predict(X_test_tf)\n",
    "y_pred_train = m.predict(X_train_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test\n",
    "sns.histplot(data=pd.DataFrame({'y_pred_test':y_pred_test, 'y_test':y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution looks fine. Negative demand has to be clamped to zero in the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.score(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE:'\n",
    "      , mean_squared_error(y_train, m.predict(X_train_tf))\n",
    "      , mean_squared_error(y_test, m.predict(X_test_tf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(m, X_train_tf, y_train, cv=5, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean(), scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No signs of overfitting so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Second approach: Hyperparameter Optimization\n",
    "### 6.1 Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
